import os
import time
import uuid
import asyncio
import gradio as gr
import torch
import opencc
from PIL import Image, ImageOps
from typing import List, Optional, Any
import threading

from transformers import (
    Qwen3VLForConditionalGeneration,
    AutoProcessor,
    AutoTokenizer,
    PreTrainedTokenizerBase,
    TextIteratorStreamer,
)

def normalize_china(text: str):
    return text.replace("ä¸­åœ‹", "ä¸­åœ‹å¤§é™¸")

def cut_to_user_question(user_text: str, model_output: str, has_image: bool = False) -> str:
    # åœ–ç‰‡æ¨¡å¼ä¸è¦è£åˆ‡
    return model_output

    
def to_traditional(text: str):
    """èª¿ç”¨ OpenCCï¼ŒæŠŠè¼¸å…¥è½‰ç‚ºç¹é«”"""
    try:
        return converter.convert(text)
    except:
        return text
from faster_whisper import WhisperModel
import edge_tts
# -------------------- åš´æ ¼ç°¡é«”åµæ¸¬ --------------------
# å¸¸è¦‹ç°¡é«”å­—ï¼ˆ2000+ å­—ä¹Ÿå¯çµ¦ä½ å®Œæ•´è¡¨ï¼Œç›®å‰çµ¦æ ¸å¿ƒå¸¸è¦‹é«˜é¢¨éšªå­—ï¼‰
SIMPLIFIED_SET = set("ä¸‡ä¸ä¸‘ä¸“ç€ä¸šä¸°ä¸ºä¸¾ä¹ˆä¹‰ä¹Œä¹ä¹”ä¹ ä¹¡è¾¹ä¹¦ä¹°ä¹±äº‰äºäº‘äº˜äºšäº§äº©äº²äºµä»†ä»…ä»ä»‘ä»“ä»ªä»¬ä¼—ä¼˜ä¼™ä¼šä¼ä¼Ÿä¼ ä¼¤ä¼¥ä¼¦ä¼§ä¼ªä¼«ä½“ä½™ä½£ä½¥ä¾ ä¾£ä¾¥ä¾¦ä¾§ä¾¨ä¾©ä¾ªä¾¬ä¿­å€ºå€¾å¬å»å¿å‚¥å€¾ä¼ å€ºä¼¤ä¼¥ä¼¦ä¼§ä½¥ä¾¥ä¾¦ä¾§ä¾¨ä¾©ä¾ªä¿¨ä¿©ä¿ªä¿«ä¿¬ä¿­å€ºå‚¨")

import re
def contains_simplified(text: str) -> bool:
    if not text:
        return False
    # è‹¥æ–‡å­—ä¸­åŒ…å«ä»»ä¸€ç°¡é«”å­— â†’ åˆ¤å®šç‚ºç°¡é«”
    return any(ch in SIMPLIFIED_SET for ch in text)

torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

# é¡¯ç¤º transformers è©³ç´°æ—¥èªŒï¼Œæ–¹ä¾¿è¨ºæ–·ä¸æ”¯æ´çš„ generation åƒæ•¸
os.environ.setdefault('TRANSFORMERS_VERBOSITY', 'info')

# -------------------- USER CONFIG --------------------
MODEL_PATH = r"D:\DATASET"
LOGO_PATH = r"D:\æœªå‘½å.png"
PORT = 7860

HAS_CUDA = torch.cuda.is_available()
DEVICE = "cuda" if HAS_CUDA else "cpu"

# -------------------- OpenCC --------------------
converter = opencc.OpenCC("s2t.json")


def to_traditional(s):
    try:
        return converter.convert(s)
    except Exception:
        return s


# -------------------- LOAD MODEL --------------------
print("Loading DATASET...")

model = Qwen3VLForConditionalGeneration.from_pretrained(
    MODEL_PATH,
    dtype=torch.float16 if HAS_CUDA else torch.float32,
    device_map="auto" if HAS_CUDA else None,
    trust_remote_code=True,
)

try:
    model.config.use_cache = True
    if hasattr(model, "generation_config"):
        model.generation_config.use_cache = True
except Exception:
    pass

processor = AutoProcessor.from_pretrained(MODEL_PATH, trust_remote_code=True)

tokenizer: PreTrainedTokenizerBase = getattr(processor, "tokenizer", None)
if tokenizer is None:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)

print("Model ready.")

strict_chat_template = """
{% for message in messages %}
<|im_start|>{{ message['role'] }}
{{ message['content'] }}
<|im_end|>
{% endfor %}
<|im_start|>assistant
"""

tokenizer.chat_template = strict_chat_template
processor.tokenizer.chat_template = strict_chat_template


# -------------------- Whisper STT --------------------
whisper_model = WhisperModel("small", device=DEVICE)


def stt_from_file_sync(path):
    if not path:
        return ""
    seg, _ = whisper_model.transcribe(path)
    return "".join([s.text for s in seg])


# -------------------- Edge-TTS --------------------
async def synthesize_edge(text, out_path):
    comm = edge_tts.Communicate(text, "zh-TW-HsiaoChenNeural")
    await comm.save(out_path)
    return out_path


def synthesize_edge_sync(text, out_path=None):
    """
    åŒæ­¥ wrapperï¼š
    - è‹¥æ²’æœ‰æ­£åœ¨é‹è¡Œçš„ event loopï¼Œç›´æ¥ asyncio.run()
    - è‹¥å·²æœ‰é‹è¡Œä¸­çš„ loopï¼ˆä¾‹å¦‚ Gradioï¼‰ï¼Œå‰‡åœ¨æ–°åŸ·è¡Œç·’å»ºç«‹æ–°çš„ loop ä¸¦åŸ·è¡Œ coroutine
    é€™æ¨£å¯ä»¥é¿å…åœ¨å·²é‹è¡Œçš„ loop ä½¿ç”¨ asyncio.run() å°è‡´éŒ¯èª¤ã€‚
    """
    if not out_path:
        out_path = f"tts_{int(time.time())}_{uuid.uuid4().hex}.mp3"

    # å˜—è©¦å–å¾—ç›®å‰ loopï¼›éƒ¨åˆ†ç’°å¢ƒæœƒå™´éŒ¯ï¼Œç”¨ try/except åŒ…ä½
    try:
        loop = asyncio.get_event_loop()
    except Exception:
        loop = None

    # è‹¥æ²’æœ‰ loop æˆ– loop æœªåœ¨é‹è¡Œï¼Œç›´æ¥ asyncio.run
    if loop is None or not loop.is_running():
        return asyncio.run(synthesize_edge(text, out_path))

    # è‹¥å·²æœ‰æ­£åœ¨é‹è¡Œçš„ loopï¼ˆå¸¸è¦‹æ–¼ Gradioï¼‰ï¼Œåœ¨æ–°åŸ·è¡Œç·’å»ºç«‹æ–°çš„ loop åŸ·è¡Œ coroutine
    result = {"out": None, "exc": None}

    def _runner():
        try:
            new_loop = asyncio.new_event_loop()
            asyncio.set_event_loop(new_loop)
            new_loop.run_until_complete(synthesize_edge(text, out_path))
            new_loop.close()
            result["out"] = out_path
        except Exception as e:
            result["exc"] = e

    t = threading.Thread(target=_runner)
    t.start()
    t.join()

    if result["exc"]:
        raise result["exc"]
    return result["out"]


# -------------------- Helpers --------------------
SYSTEM_PROMPT = (
    "ä½¿ç”¨è€…è¼¸å…¥ä¸­æ–‡æ™‚ï¼Œå‹™å¿…ä½¿ç”¨ç¹é«”ä¸­æ–‡å›è¦†ï¼›è‹¥ä½¿ç”¨è€…ä½¿ç”¨è‹±æ–‡ï¼Œå‰‡ä»¥è‹±æ–‡å›è¦†ã€‚"
    "ä»»ä½•ç°¡é«”å­—éƒ½å¿…é ˆè½‰ç‚ºç¹é«”å¾Œå†è¼¸å‡ºã€‚"
    "è‹¥ä½¿ç”¨è€…è©¢å•æ³•å¾‹å•é¡Œï¼Œåš´ç¦å‡ºç¾'æ³•å¾‹ä¾æ“šåƒè€ƒï¼ˆä¸­åœ‹å¤§é™¸å¤§é™¸ç¯„åœ)'ç­‰å­—æ¨£ã€‚"
    "è‹¥ä½¿ç”¨è€…å«ä½ ç”Ÿæˆç¨‹å¼ç¢¼ï¼Œåš´ç¦ç”Ÿæˆç¨‹å¼ç¢¼ä»¥å¤–çš„ã€‚"
)


def detect_language(text):
    if not text:
        return "zh"
    a = sum(1 for c in text if ord(c) < 128)
    return "en" if a / max(len(text), 1) > 0.6 else "zh"


# ---------- åœ–ç‰‡æ­£è¦åŒ– helper ----------
def normalize_image(img: Image.Image, max_side: int = 1280) -> Image.Image:
    """
    - è½‰ RGB
    - è™•ç† EXIF æ–¹å‘
    - è‹¥åœ–ç‰‡å¤ªå¤§ï¼Œä¾æ¯”ä¾‹ç¸®å°ï¼ˆé¿å… GPU memory çˆ†æ‰ï¼‰
    """
    try:
        img = ImageOps.exif_transpose(img)
    except Exception:
        pass
    if img.mode != "RGB":
        img = img.convert("RGB")
    # ç¸®æ”¾ï¼ˆè‹¥ä»»ä¸€é‚Šå¤§æ–¼ max_sideï¼Œç­‰æ¯”ç¸®ï¼‰
    w, h = img.size
    if max(w, h) > max_side:
        scale = max_side / float(max(w, h))
        new_w = int(w * scale)
        new_h = int(h * scale)
        img = img.resize((new_w, new_h), Image.LANCZOS)
    return img


# ----------------------------------------------------
#               Qwen3-VL æ¨è«–æ ¸å¿ƒï¼ˆæ•´åˆç‰ˆï¼‰
# ----------------------------------------------------

def _build_messages(user_text: str, images: List[Image.Image], history: List[List[str]]):
    """
    å»ºç«‹ messages listï¼š
    - system message
    - è·Ÿéš¨æ­·å²ï¼ˆæ¯ä¸€å€‹ turn æœŸæœ› history ç‚º [user, assistant]ï¼‰
    - æœ€å¾Œä¸€å€‹ user messageï¼Œå…§å®¹å¯åŒ…å«å¤šå¼µ image èˆ‡ text
    """
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    text_prompt = processor.apply_chat_template(messages, tokenize=False)
    
    if history:
        # history æ˜¯ list of [user, assistant]
        for turn in history:
            if not isinstance(turn, (list, tuple)) or len(turn) < 1:
                continue
            user_msg = turn[0] if len(turn) > 0 else ""
            assistant_msg = turn[1] if len(turn) > 1 else ""

            # user
            messages.append({"role": "user", "content": user_msg})
            # assistant (å¦‚æœæœ‰)
            if assistant_msg:
                messages.append({"role": "assistant", "content": assistant_msg})

    # build content for current user (images + text)
    content = []
    for img in images or []:
        # ensure PIL Image and normalize
        try:
            nimg = normalize_image(img)
        except Exception:
            nimg = img
        content.append({"type": "image", "image": nimg})
    content.append({"type": "text", "text": user_text or ""})

    messages.append({"role": "user", "content": content})
    return messages


def _processor_prepare_inputs(messages):
    """
    å˜—è©¦ä»¥è¼ƒæ–°æ–¹æ³•è½‰æ› messagesï¼ˆapply_chat_templateï¼‰ï¼Œè‹¥ processor æ²’æœ‰è©²æ–¹æ³•å‰‡ fallback åˆ°
    processor(messages=...)ã€‚æœ€å¾Œå›å‚³ tensor dict ä¸¦ç§»åˆ° model.deviceã€‚

    ç‚ºäº†é¿å… image-token mismatchï¼Œç•¶ messages å« image æ™‚ï¼š
    1) ç”¨ apply_chat_template(..., tokenize=False) å–å¾— prompt_text
    2) å†å‘¼å« processor(text=prompt_text, images=imgs, return_tensors="pt")
    """
    # æª¢æŸ¥ messages ä¸­æ˜¯å¦å« image
    imgs = []
    try:
        last = messages[-1]["content"]
        for c in last:
            if isinstance(c, dict) and c.get("type") == "image":
                img_obj = c.get("image")
                if img_obj is not None:
                    try:
                        img_obj = normalize_image(img_obj)
                    except Exception:
                        pass
                    imgs.append(img_obj)
    except Exception:
        imgs = []

    # Note: ä¸åŒç‰ˆæœ¬çš„ processor API ä¸åŒï¼Œé€™è£¡æ¡ç”¨ try/except å®¹éŒ¯
    try:
        # è‹¥åŒ…å« imageï¼Œæˆ‘å€‘å…ˆç”¨ tokenize=False å–å¾— prompt_textï¼Œå†æŠŠ text+images ä¸€èµ·å‚³çµ¦ processor
        if imgs:
            prompt_text = processor.apply_chat_template(messages, tokenize=False)
            inputs = processor(text=prompt_text, images=imgs, return_tensors="pt")
        else:
            # æ²’æœ‰ images æ™‚ç›´æ¥ç”¨ tokenize=True çš„å¿«æ·æ–¹å¼
            inputs = processor.apply_chat_template(
                messages,
                add_generation_prompt=True,
                tokenize=True,
                return_tensors="pt"
            )
    except Exception:
        # fallback: æœ‰äº›ç‰ˆæœ¬æ”¯æ´ç›´æ¥å‚³ messages
        try:
            # æ­£ç¢ºï¼šå…ˆå°‡ messages è½‰æˆæ–‡å­—æ¨¡æ¿
            inputs = processor.apply_chat_template(
                messages,
                add_generation_prompt=True,
                tokenize=True,
                return_tensors="pt"
            ).to(model.device)

        except TypeError:
            # å† fallbackï¼šç”¨ text/images åˆ†é–‹
            # å˜—è©¦æŠŠæœ€å¾Œ user content æ‹†å› text & images
            try:
                last = messages[-1]["content"]
                imgs2 = [c["image"] for c in last if isinstance(c, dict) and c.get("type") == "image"]
                texts = [c["text"] for c in last if isinstance(c, dict) and c.get("type") == "text"]
                text = texts[0] if texts else ""
                if imgs2:
                    imgs2 = [normalize_image(i) for i in imgs2]
                inputs = processor(
                    images=imgs2 if imgs2 else None,
                    text=text,
                    padding="max_length",
                    max_length=512,
                    return_tensors="pt"
                )
            except Exception:
                # æœ€å¾Œè·Œå›åˆ°æŠŠæ‰€æœ‰ messages ç•¶æˆ text
                prompt_text = ""
                try:
                    prompt_text = processor.apply_chat_template(messages, tokenize=False)
                except Exception:
                    prompt_text = str(messages)
                inputs = processor(text=prompt_text, return_tensors="pt")

    # move tensors to model device if returned as dict of tensors
    if isinstance(inputs, dict):
        for k, v in list(inputs.items()):
            try:
                if hasattr(v, "to"):
                    inputs[k] = v.to(model.device)
            except Exception:
                pass
    return inputs


def safe_model_generate(model, *args, **kwargs):
    """Robust wrapper for model.generate.

    - Removes unsupported generation kwargs ('temperature','top_p','top_k').
    - Recursively moves any torch.Tensor in args/kwargs to the model device
      to avoid device-mismatch RuntimeError.
    - On device-related RuntimeError, retries once after forcing device move.
    """
    # filter out unsupported/invalid keys
    for k in ("temperature", "top_p", "top_k"):
        if k in kwargs:
            kwargs.pop(k)

    def _move_to_device(obj, device):
        # move tensors recursively; leave other objects untouched
        if isinstance(obj, torch.Tensor):
            try:
                return obj.to(device)
            except Exception:
                return obj
        elif isinstance(obj, dict):
            return {k: _move_to_device(v, device) for k, v in obj.items()}
        elif isinstance(obj, (list, tuple)):
            moved = [_move_to_device(v, device) for v in obj]
            return type(obj)(moved)
        else:
            return obj

    # determine model device
    try:
        model_device = next(model.parameters()).device
    except Exception:
        model_device = getattr(model, 'device', torch.device('cuda' if torch.cuda.is_available() else 'cpu'))

    new_args = tuple(_move_to_device(a, model_device) for a in args)
    new_kwargs = {k: _move_to_device(v, model_device) for k, v in kwargs.items()}

    try:
        return model.generate(*new_args, **new_kwargs)
    except RuntimeError as e:
        msg = str(e)
        # if device mismatch or image-token mismatch, try one more time after forcing move
        if ('Expected all tensors to be on the same device' in msg) or ('Image features and image tokens do not match' in msg):
            try:
                # recompute device from model and move again
                try:
                    model_device = next(model.parameters()).device
                except Exception:
                    model_device = getattr(model, 'device', model_device)
                new_args = tuple(_move_to_device(a, model_device) for a in new_args)
                new_kwargs = {k: _move_to_device(v, model_device) for k, v in new_kwargs.items()}
                return model.generate(*new_args, **new_kwargs)
            except Exception:
                # fall through to raise original
                pass
        raise


def run_model(user_text: str, images: List[Image.Image], history):
    # ä½¿ç”¨ message-builder + processor wrapper ç”¢ç”Ÿæ­£ç¢ºçš„ inputs
    messages = _build_messages(user_text, images or [], history)
    inputs = _processor_prepare_inputs(messages)

    # å‘¼å« generateï¼Œè‹¥ model æˆ– processor æœ‰ç›¸å®¹æ€§å•é¡Œï¼Œæœƒç”± safe_model_generate éæ¿¾ä¸æ”¯æ´çš„ç”Ÿæˆåƒæ•¸
    output = safe_model_generate(
        model,
        **inputs,
        max_new_tokens=1024,
        do_sample=False,
        use_cache=True
    )

    # output å¯èƒ½æ˜¯ tensor of shape (1, seq_len)
    if isinstance(output, torch.Tensor):
        seq = output[0]
    else:
        try:
            seq = output[0]
        except Exception:
            seq = output

    try:
        text = tokenizer.decode(seq, skip_special_tokens=True)
    except Exception:
        # è‹¥ decode å¤±æ•—ï¼Œå˜—è©¦è½‰æˆå­—ä¸²
        try:
            if isinstance(seq, torch.Tensor):
                seq_list = seq.cpu().tolist()
                text = tokenizer.decode(seq_list, skip_special_tokens=True)
            else:
                text = str(seq)
        except Exception:
            text = str(seq)

    # ç§»é™¤ prompt
    if "Assistant:" in text:
        text = text.split("Assistant:")[-1].strip()

    return to_traditional(text)


def stream_model(user_text: str, images: List[Image.Image], history): 
    # ==== çµ„ Prompt & ä½¿ç”¨ _processor_prepare_inputs ä¿è­‰åœ–ç‰‡è¢«åŒ…å« ====
    msgs = _build_messages(user_text, images or [], history)
    inputs = _processor_prepare_inputs(msgs)

    # ==== Streamer ====
    streamer = TextIteratorStreamer(
        tokenizer,
        skip_special_tokens=True,
        skip_prompt=True,
    )

    def gen():
        safe_model_generate(
            model,
            **inputs,
            max_new_tokens=150,
            repetition_penalty=1.2,  # é™ä½æ¨¡å‹é‡è¤‡æ“´å¯«å‚¾å‘
            do_sample=False,
            no_repeat_ngram_size=3,        # é˜»æ­¢æ¨¡å‹é‡è¤‡ç‰‡èªæ‹–é•·æ–‡æœ¬
            streamer=streamer,
            use_cache=True
        )

    threading.Thread(target=gen).start()

    buffer = ""
    for chunk in streamer:
        buffer += chunk

    # ====== æ–°å¢ï¼šåªå…è¨±èˆ‡ä½¿ç”¨è€…å•é¡Œç›¸é—œçš„è¼¸å‡º ======
        cleaned = cut_to_user_question(user_text, buffer, has_image=bool(images))
        cleaned = to_traditional(cleaned)
        cleaned = normalize_china(cleaned)

        if "\n" in buffer:
            yield cleaned
            buffer = ""


    # ====== é‚„æœ‰æ®˜ç•™å­—ä¸² â†’ æœ€å¾Œä¸€æ¬¡è¼¸å‡º ======
    if buffer:
        buffer = to_traditional(buffer)
        buffer = normalize_china(buffer)
        yield buffer

# ----------------------------------------------------
#                     Gradio UI
# ----------------------------------------------------
css = """
:root { --bg:#0b1020 }
body { background:var(--bg); color:#e6eef6 }
#chat-area { height:740px }
"""

with gr.Blocks(title="DeepChat", css=css) as demo:

    gr.Markdown("<h2 style='text-align:center;color:white'>DeepChat</h2>")

    chatbot = gr.Chatbot(label="å°è©±ç´€éŒ„", elem_id="chat-area")

    with gr.Row():
        input_box = gr.Textbox(show_label=False, placeholder="è¼¸å…¥è¨Šæ¯â€¦")
        image_input = gr.File(
            label="ä¸Šå‚³åœ–ç‰‡ï¼ˆå¯å¤šå¼µï¼‰",
            file_types=["image"],
            file_count="multiple"
        )

    mic_btn = gr.Audio(sources=["microphone"], type="filepath", label="ğŸ¤")
    send_btn = gr.Button("é€å‡º")

    with gr.Row():
        clear_btn = gr.Button("æ¸…é™¤å°è©±")
        tts_btn = gr.Button("èªéŸ³æ’­å‡º")

    # ------------ Submitï¼ˆgeneratorï¼Œæ”¯æ´ streamingï¼‰ ------------
    def _open_file_to_pil(f: Any):
        # f å¯èƒ½æ˜¯ str path / dict / tempfile-like
        if f is None:
            return None
        try:
            if isinstance(f, str):
                img = Image.open(f)
                return normalize_image(img)
            # gradio may give a dict with 'name' or 'file'
            if isinstance(f, dict):
                path = f.get("name") or f.get("tmp_path") or f.get("file")
                if path:
                    img = Image.open(path)
                    return normalize_image(img)
            # file-like object
            if hasattr(f, "name"):
                img = Image.open(f.name)
                return normalize_image(img)
        except Exception:
            return None
        return None

    def submit_fn(user_text, user_images, chat_history):
                # ---------- å¼·åˆ¶ç¹é«”æ©Ÿåˆ¶ï¼ˆä¸å‹•ä½ ä»»ä½•åœ–ç‰‡é‚è¼¯ï¼‰ ----------
        if contains_simplified(user_text):
            user_text = to_traditional(user_text)

        # ---- ä¿è­·ï¼šç¢ºä¿ chat_history ä¸æœƒè¢« streaming æ±™æŸ“ ----
        if chat_history is None:
            chat_history = []
        else:
            chat_history = [[u, a] for (u, a) in chat_history]

        # æ–°å¢ä½¿ç”¨è€…è¨Šæ¯
        new_item = [user_text, ""]
        chat_history = chat_history + [new_item]

        yield chat_history, "", None

        # ---- å®Œå…¨ä¸å‹•ä½ åœ–ç‰‡è™•ç†é‚è¼¯ ----
        pil_imgs = []
        if user_images:
            imgs = user_images if isinstance(user_images, (list, tuple)) else [user_images]
            for f in imgs:
                pil = _open_file_to_pil(f)  # ä¿ç•™åŸæœ¬ä½ çš„è™•ç†
                if pil is not None:
                    pil_imgs.append(pil)

        # ---- local bufferï¼ˆä¸æ±¡æŸ“ historyï¼‰----
        assistant_text = ""

        try:
            # é—œéµï¼šä¸æŠŠå°šæœªå¡«æ»¿çš„æœ€å¾Œä¸€è¡Œ history å‚³é€² streaming
            for chunk in stream_model(user_text, pil_imgs, chat_history[:-1]):
                assistant_text += chunk
                chat_history[-1][1] = assistant_text
                yield chat_history, "", None

        except Exception as e:
            err = f"(æ¨è«–å¤±æ•—ï¼š{e})"
            chat_history[-1][1] = err
            yield chat_history, "", None
            return

        # æœ€çµ‚ç”¢ç‰©å¡«å…¥
        chat_history[-1][1] = assistant_text
        yield chat_history, "", None

    # ç¶å®šæŒ‰éˆ•èˆ‡ Enterï¼ˆå¿…é ˆåœ¨ with ç¯„åœå…§ï¼‰
    send_btn.click(
        submit_fn,
        inputs=[input_box, image_input, chatbot],
        outputs=[chatbot, input_box, image_input]
    )

    input_box.submit(
        submit_fn,
        inputs=[input_box, image_input, chatbot],
        outputs=[chatbot, input_box, image_input]
    )

    # ------------ Mic â†’ STT â†’ Auto send ------------
    def mic_to_text(fp, history):
        if history is None:
            history = []

        if not fp:
            return history, ""

        text = to_traditional(stt_from_file_sync(fp))

        # ç›´æ¥ç”¨ run_modelï¼ˆé streamingï¼‰
        reply = run_model(text, [], history)

        history = history + [[text, reply]]
        return history, ""

    mic_btn.change(
        mic_to_text,
        inputs=[mic_btn, chatbot],
        outputs=[chatbot, input_box]
    )

    # ------------ TTS ------------
    def tts_play(history):
        if not history:
            return None
        # history item æ˜¯ [user, assistant]
        last = history[-1]
        if isinstance(last, (list, tuple)) and len(last) > 1:
            msg = last[1]
        elif isinstance(last, dict):
            msg = last.get("content", "")
        else:
            msg = ""
        out = f"tts_{time.time()}.mp3"
        return synthesize_edge_sync(msg, out)

    tts_btn.click(
        tts_play,
        inputs=[chatbot],
        outputs=gr.Audio(type="filepath")
    )

    # ------------ Clear ------------
    clear_btn.click(lambda: [], None, chatbot)

# -------------------- Launch --------------------
if __name__ == "__main__":
    demo.launch(server_name="127.0.0.1", server_port=PORT, share=False)
